{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораториска вежба 5–Класификација на графови"
      ],
      "metadata": {
        "id": "HPAHixKcsTn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Податочно множество –DDПодаточното множество DD се состои од1 178 протеински структури претставени како графови. Јазлите претставуваат аминокиселини. Два јазли се поврзани ако растојанието  помеѓу  соодветните  аминокиселини  е  помало  од  6ангстроми  (0.6 нанометри).Целта е да се одреди дали дадена протеинска структура е ензим или не. За вчитување на множеството искористете го следниот код:from torch_geometric.datasets import TUDatasetdata = TUDataset(root='../data/TUDataset/DD', name='DD', use_node_attr=True)"
      ],
      "metadata": {
        "id": "MTl0KfuTsMnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric\n",
        "\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OHQbu7XislcI",
        "outputId": "b43e951f-e086-4410-8962-fad1132ad081"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.10/dist-packages (0.4.0+pt23cu121)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt23cu121)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt23cu121)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt23cu121)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt23cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jRsd_dHvsCyG"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch_geometric.nn import to_hetero, GCNConv, Linear, SAGEConv\n",
        "from torch_geometric.datasets import IMDB, Actor\n",
        "from torch_geometric.loader import NeighborLoader, DataLoader\n",
        "# from torch_geometric.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import torch\n",
        "from torch.nn.functional import dropout\n",
        "from torch_geometric.nn import to_hetero\n",
        "from torch_geometric.nn import Linear, SAGEConv, global_mean_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Од 8мата аудиториска вежба\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            loss = criterion(out, batch.y)\n",
        "            loss.backward()\n",
        "            train_loss = loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Loss: {train_loss:.4f}')\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            model.eval()\n",
        "\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            loss = criterion(out, batch.y)\n",
        "            val_loss = loss.item()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv((-1, -1), 64)\n",
        "        self.conv2 = SAGEConv((-1, -1), 128)\n",
        "        self.conv3 = SAGEConv((-1, -1), 64)\n",
        "\n",
        "        self.linear1 = Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = dropout(x, p=0.5, training=self.training)\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "dnXRVAH02I4k"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TUDataset(root='../data/TUDataset/DD', name='DD', use_node_attr=True)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ2apnCB2P18",
        "outputId": "80d523b9-f780-4459-e3a0-18779e7fbf51"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DD(1178)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задачи"
      ],
      "metadata": {
        "id": "72zvPPIXnUX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 1–Класификација на графовисо GraphSAGE (25поени) Направете модел за класификација на графовикој го имплементира GraphSAGE. Првиот дел на моделот се 2 или 3 слоеви во GraphSAGE, по што се надоврзуваат 1, 2  или  3  скриени  FC  слоеви  за  класификација  на графовите.  Направете класификација  на графовитепреку  споредба  на  различните  големини  и комбинации  на  слоеви  на  моделите.Поделете  ги  графовите  на  графови  за тренирање,  валидација  и  тестирање.Истренираниот  модел  евалуирајте  го  со метриките: точност(accuracy_score),  прецизност(precision_score),  одзив(recall_score)и F1-мерка(f1_score)"
      ],
      "metadata": {
        "id": "ITkjZcpknVtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "7AhfsesbnaNQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GraphSAGE(num_classes=dataset.num_classes)\n",
        "optimizer = SGD(model.parameters(), lr=0.0001)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "KZ6D0gC34AJQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, val_loader, optimizer, criterion, 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-x1NY6-7nGn",
        "outputId": "2deac5d0-0960-442e-b5a3-770cfc04879c",
        "collapsed": true
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Step: 000, Loss: 0.7096\n",
            "Epoch: 000, Step: 001, Loss: 0.7065\n",
            "Epoch: 000, Step: 002, Loss: 0.6962\n",
            "Epoch: 000, Step: 003, Loss: 0.6971\n",
            "Epoch: 000, Step: 004, Loss: 0.7044\n",
            "Epoch: 000, Step: 005, Loss: 0.7000\n",
            "Epoch: 000, Step: 006, Loss: 0.6960\n",
            "Epoch: 000, Step: 007, Loss: 0.7017\n",
            "Epoch: 000, Step: 008, Loss: 0.7064\n",
            "Epoch: 000, Step: 009, Loss: 0.6979\n",
            "Epoch: 000, Step: 010, Loss: 0.6933\n",
            "Epoch: 000, Step: 011, Loss: 0.6914\n",
            "Epoch: 000, Step: 012, Loss: 0.6913\n",
            "Epoch: 000, Step: 013, Loss: 0.6951\n",
            "Epoch: 000, Step: 014, Loss: 0.7268\n",
            "Epoch: 000, Step: 015, Loss: 0.7082\n",
            "Epoch: 000, Step: 016, Loss: 0.7147\n",
            "Epoch: 000, Step: 017, Loss: 0.6982\n",
            "Epoch: 000, Step: 018, Loss: 0.6932\n",
            "Epoch: 000, Step: 019, Loss: 0.6977\n",
            "Epoch: 000, Step: 020, Loss: 0.6931\n",
            "Epoch: 000, Step: 021, Loss: 0.6970\n",
            "Epoch: 000, Step: 022, Loss: 0.6933\n",
            "Epoch: 000, Step: 023, Loss: 0.7182\n",
            "Epoch: 000, Step: 024, Loss: 0.6879\n",
            "Epoch: 000, Step: 025, Loss: 0.6998\n",
            "Epoch: 000, Step: 026, Loss: 0.6854\n",
            "Epoch: 000, Step: 027, Loss: 0.7043\n",
            "Epoch: 000, Step: 028, Loss: 0.6993\n",
            "Epoch: 000, Step: 029, Loss: 0.6919\n",
            "Epoch: 000, Step: 000, Val Loss: 0.6875\n",
            "Epoch: 000, Step: 001, Val Loss: 0.6978\n",
            "Epoch: 000, Step: 002, Val Loss: 0.6958\n",
            "Epoch: 000, Step: 003, Val Loss: 0.6936\n",
            "Epoch: 001, Step: 000, Loss: 0.6936\n",
            "Epoch: 001, Step: 001, Loss: 0.7008\n",
            "Epoch: 001, Step: 002, Loss: 0.7036\n",
            "Epoch: 001, Step: 003, Loss: 0.6941\n",
            "Epoch: 001, Step: 004, Loss: 0.7032\n",
            "Epoch: 001, Step: 005, Loss: 0.6975\n",
            "Epoch: 001, Step: 006, Loss: 0.7036\n",
            "Epoch: 001, Step: 007, Loss: 0.7034\n",
            "Epoch: 001, Step: 008, Loss: 0.7079\n",
            "Epoch: 001, Step: 009, Loss: 0.7018\n",
            "Epoch: 001, Step: 010, Loss: 0.6993\n",
            "Epoch: 001, Step: 011, Loss: 0.6870\n",
            "Epoch: 001, Step: 012, Loss: 0.7056\n",
            "Epoch: 001, Step: 013, Loss: 0.7028\n",
            "Epoch: 001, Step: 014, Loss: 0.6952\n",
            "Epoch: 001, Step: 015, Loss: 0.6932\n",
            "Epoch: 001, Step: 016, Loss: 0.6960\n",
            "Epoch: 001, Step: 017, Loss: 0.6892\n",
            "Epoch: 001, Step: 018, Loss: 0.6967\n",
            "Epoch: 001, Step: 019, Loss: 0.6952\n",
            "Epoch: 001, Step: 020, Loss: 0.6968\n",
            "Epoch: 001, Step: 021, Loss: 0.7004\n",
            "Epoch: 001, Step: 022, Loss: 0.6949\n",
            "Epoch: 001, Step: 023, Loss: 0.7042\n",
            "Epoch: 001, Step: 024, Loss: 0.7052\n",
            "Epoch: 001, Step: 025, Loss: 0.7047\n",
            "Epoch: 001, Step: 026, Loss: 0.7070\n",
            "Epoch: 001, Step: 027, Loss: 0.6992\n",
            "Epoch: 001, Step: 028, Loss: 0.7027\n",
            "Epoch: 001, Step: 029, Loss: 0.6997\n",
            "Epoch: 001, Step: 000, Val Loss: 0.6957\n",
            "Epoch: 001, Step: 001, Val Loss: 0.6955\n",
            "Epoch: 001, Step: 002, Val Loss: 0.6976\n",
            "Epoch: 001, Step: 003, Val Loss: 0.6820\n",
            "Epoch: 002, Step: 000, Loss: 0.6985\n",
            "Epoch: 002, Step: 001, Loss: 0.6951\n",
            "Epoch: 002, Step: 002, Loss: 0.6914\n",
            "Epoch: 002, Step: 003, Loss: 0.7068\n",
            "Epoch: 002, Step: 004, Loss: 0.6991\n",
            "Epoch: 002, Step: 005, Loss: 0.7030\n",
            "Epoch: 002, Step: 006, Loss: 0.7037\n",
            "Epoch: 002, Step: 007, Loss: 0.6972\n",
            "Epoch: 002, Step: 008, Loss: 0.7049\n",
            "Epoch: 002, Step: 009, Loss: 0.6928\n",
            "Epoch: 002, Step: 010, Loss: 0.6989\n",
            "Epoch: 002, Step: 011, Loss: 0.7108\n",
            "Epoch: 002, Step: 012, Loss: 0.6943\n",
            "Epoch: 002, Step: 013, Loss: 0.6995\n",
            "Epoch: 002, Step: 014, Loss: 0.7020\n",
            "Epoch: 002, Step: 015, Loss: 0.7103\n",
            "Epoch: 002, Step: 016, Loss: 0.7083\n",
            "Epoch: 002, Step: 017, Loss: 0.6975\n",
            "Epoch: 002, Step: 018, Loss: 0.6986\n",
            "Epoch: 002, Step: 019, Loss: 0.6926\n",
            "Epoch: 002, Step: 020, Loss: 0.7000\n",
            "Epoch: 002, Step: 021, Loss: 0.7070\n",
            "Epoch: 002, Step: 022, Loss: 0.7062\n",
            "Epoch: 002, Step: 023, Loss: 0.6984\n",
            "Epoch: 002, Step: 024, Loss: 0.7024\n",
            "Epoch: 002, Step: 025, Loss: 0.6865\n",
            "Epoch: 002, Step: 026, Loss: 0.6921\n",
            "Epoch: 002, Step: 027, Loss: 0.6891\n",
            "Epoch: 002, Step: 028, Loss: 0.6915\n",
            "Epoch: 002, Step: 029, Loss: 0.7104\n",
            "Epoch: 002, Step: 000, Val Loss: 0.6993\n",
            "Epoch: 002, Step: 001, Val Loss: 0.6936\n",
            "Epoch: 002, Step: 002, Val Loss: 0.6899\n",
            "Epoch: 002, Step: 003, Val Loss: 0.6908\n",
            "Epoch: 003, Step: 000, Loss: 0.6920\n",
            "Epoch: 003, Step: 001, Loss: 0.7014\n",
            "Epoch: 003, Step: 002, Loss: 0.6897\n",
            "Epoch: 003, Step: 003, Loss: 0.7029\n",
            "Epoch: 003, Step: 004, Loss: 0.6943\n",
            "Epoch: 003, Step: 005, Loss: 0.6929\n",
            "Epoch: 003, Step: 006, Loss: 0.7091\n",
            "Epoch: 003, Step: 007, Loss: 0.6969\n",
            "Epoch: 003, Step: 008, Loss: 0.6980\n",
            "Epoch: 003, Step: 009, Loss: 0.6979\n",
            "Epoch: 003, Step: 010, Loss: 0.6975\n",
            "Epoch: 003, Step: 011, Loss: 0.7052\n",
            "Epoch: 003, Step: 012, Loss: 0.6955\n",
            "Epoch: 003, Step: 013, Loss: 0.7005\n",
            "Epoch: 003, Step: 014, Loss: 0.6961\n",
            "Epoch: 003, Step: 015, Loss: 0.6894\n",
            "Epoch: 003, Step: 016, Loss: 0.7057\n",
            "Epoch: 003, Step: 017, Loss: 0.6978\n",
            "Epoch: 003, Step: 018, Loss: 0.6989\n",
            "Epoch: 003, Step: 019, Loss: 0.6945\n",
            "Epoch: 003, Step: 020, Loss: 0.7019\n",
            "Epoch: 003, Step: 021, Loss: 0.6976\n",
            "Epoch: 003, Step: 022, Loss: 0.7013\n",
            "Epoch: 003, Step: 023, Loss: 0.6967\n",
            "Epoch: 003, Step: 024, Loss: 0.6973\n",
            "Epoch: 003, Step: 025, Loss: 0.6916\n",
            "Epoch: 003, Step: 026, Loss: 0.6981\n",
            "Epoch: 003, Step: 027, Loss: 0.7094\n",
            "Epoch: 003, Step: 028, Loss: 0.6979\n",
            "Epoch: 003, Step: 029, Loss: 0.7208\n",
            "Epoch: 003, Step: 000, Val Loss: 0.6934\n",
            "Epoch: 003, Step: 001, Val Loss: 0.6918\n",
            "Epoch: 003, Step: 002, Val Loss: 0.6955\n",
            "Epoch: 003, Step: 003, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 000, Loss: 0.6984\n",
            "Epoch: 004, Step: 001, Loss: 0.6900\n",
            "Epoch: 004, Step: 002, Loss: 0.7081\n",
            "Epoch: 004, Step: 003, Loss: 0.6801\n",
            "Epoch: 004, Step: 004, Loss: 0.6991\n",
            "Epoch: 004, Step: 005, Loss: 0.6997\n",
            "Epoch: 004, Step: 006, Loss: 0.6941\n",
            "Epoch: 004, Step: 007, Loss: 0.7029\n",
            "Epoch: 004, Step: 008, Loss: 0.6959\n",
            "Epoch: 004, Step: 009, Loss: 0.7082\n",
            "Epoch: 004, Step: 010, Loss: 0.6954\n",
            "Epoch: 004, Step: 011, Loss: 0.6986\n",
            "Epoch: 004, Step: 012, Loss: 0.7050\n",
            "Epoch: 004, Step: 013, Loss: 0.6977\n",
            "Epoch: 004, Step: 014, Loss: 0.6892\n",
            "Epoch: 004, Step: 015, Loss: 0.7030\n",
            "Epoch: 004, Step: 016, Loss: 0.6967\n",
            "Epoch: 004, Step: 017, Loss: 0.7057\n",
            "Epoch: 004, Step: 018, Loss: 0.6974\n",
            "Epoch: 004, Step: 019, Loss: 0.6951\n",
            "Epoch: 004, Step: 020, Loss: 0.7028\n",
            "Epoch: 004, Step: 021, Loss: 0.7079\n",
            "Epoch: 004, Step: 022, Loss: 0.7122\n",
            "Epoch: 004, Step: 023, Loss: 0.6983\n",
            "Epoch: 004, Step: 024, Loss: 0.6993\n",
            "Epoch: 004, Step: 025, Loss: 0.6966\n",
            "Epoch: 004, Step: 026, Loss: 0.7107\n",
            "Epoch: 004, Step: 027, Loss: 0.7016\n",
            "Epoch: 004, Step: 028, Loss: 0.7049\n",
            "Epoch: 004, Step: 029, Loss: 0.6774\n",
            "Epoch: 004, Step: 000, Val Loss: 0.6975\n",
            "Epoch: 004, Step: 001, Val Loss: 0.6934\n",
            "Epoch: 004, Step: 002, Val Loss: 0.6898\n",
            "Epoch: 004, Step: 003, Val Loss: 0.6937\n",
            "Epoch: 005, Step: 000, Loss: 0.6981\n",
            "Epoch: 005, Step: 001, Loss: 0.6974\n",
            "Epoch: 005, Step: 002, Loss: 0.7007\n",
            "Epoch: 005, Step: 003, Loss: 0.7076\n",
            "Epoch: 005, Step: 004, Loss: 0.6941\n",
            "Epoch: 005, Step: 005, Loss: 0.7078\n",
            "Epoch: 005, Step: 006, Loss: 0.6932\n",
            "Epoch: 005, Step: 007, Loss: 0.6902\n",
            "Epoch: 005, Step: 008, Loss: 0.6975\n",
            "Epoch: 005, Step: 009, Loss: 0.6931\n",
            "Epoch: 005, Step: 010, Loss: 0.7000\n",
            "Epoch: 005, Step: 011, Loss: 0.6903\n",
            "Epoch: 005, Step: 012, Loss: 0.7031\n",
            "Epoch: 005, Step: 013, Loss: 0.6925\n",
            "Epoch: 005, Step: 014, Loss: 0.7021\n",
            "Epoch: 005, Step: 015, Loss: 0.7022\n",
            "Epoch: 005, Step: 016, Loss: 0.6943\n",
            "Epoch: 005, Step: 017, Loss: 0.6980\n",
            "Epoch: 005, Step: 018, Loss: 0.7097\n",
            "Epoch: 005, Step: 019, Loss: 0.7022\n",
            "Epoch: 005, Step: 020, Loss: 0.6879\n",
            "Epoch: 005, Step: 021, Loss: 0.6887\n",
            "Epoch: 005, Step: 022, Loss: 0.6903\n",
            "Epoch: 005, Step: 023, Loss: 0.6964\n",
            "Epoch: 005, Step: 024, Loss: 0.7076\n",
            "Epoch: 005, Step: 025, Loss: 0.7026\n",
            "Epoch: 005, Step: 026, Loss: 0.6945\n",
            "Epoch: 005, Step: 027, Loss: 0.6988\n",
            "Epoch: 005, Step: 028, Loss: 0.7008\n",
            "Epoch: 005, Step: 029, Loss: 0.7073\n",
            "Epoch: 005, Step: 000, Val Loss: 0.7028\n",
            "Epoch: 005, Step: 001, Val Loss: 0.6899\n",
            "Epoch: 005, Step: 002, Val Loss: 0.6899\n",
            "Epoch: 005, Step: 003, Val Loss: 0.6908\n",
            "Epoch: 006, Step: 000, Loss: 0.7034\n",
            "Epoch: 006, Step: 001, Loss: 0.6887\n",
            "Epoch: 006, Step: 002, Loss: 0.7029\n",
            "Epoch: 006, Step: 003, Loss: 0.6963\n",
            "Epoch: 006, Step: 004, Loss: 0.7020\n",
            "Epoch: 006, Step: 005, Loss: 0.7033\n",
            "Epoch: 006, Step: 006, Loss: 0.7091\n",
            "Epoch: 006, Step: 007, Loss: 0.6924\n",
            "Epoch: 006, Step: 008, Loss: 0.7073\n",
            "Epoch: 006, Step: 009, Loss: 0.6992\n",
            "Epoch: 006, Step: 010, Loss: 0.6984\n",
            "Epoch: 006, Step: 011, Loss: 0.6923\n",
            "Epoch: 006, Step: 012, Loss: 0.7024\n",
            "Epoch: 006, Step: 013, Loss: 0.6806\n",
            "Epoch: 006, Step: 014, Loss: 0.6999\n",
            "Epoch: 006, Step: 015, Loss: 0.6912\n",
            "Epoch: 006, Step: 016, Loss: 0.6921\n",
            "Epoch: 006, Step: 017, Loss: 0.6988\n",
            "Epoch: 006, Step: 018, Loss: 0.7002\n",
            "Epoch: 006, Step: 019, Loss: 0.6954\n",
            "Epoch: 006, Step: 020, Loss: 0.7056\n",
            "Epoch: 006, Step: 021, Loss: 0.6928\n",
            "Epoch: 006, Step: 022, Loss: 0.7051\n",
            "Epoch: 006, Step: 023, Loss: 0.7047\n",
            "Epoch: 006, Step: 024, Loss: 0.7061\n",
            "Epoch: 006, Step: 025, Loss: 0.7019\n",
            "Epoch: 006, Step: 026, Loss: 0.7043\n",
            "Epoch: 006, Step: 027, Loss: 0.7005\n",
            "Epoch: 006, Step: 028, Loss: 0.6929\n",
            "Epoch: 006, Step: 029, Loss: 0.7016\n",
            "Epoch: 006, Step: 000, Val Loss: 0.6916\n",
            "Epoch: 006, Step: 001, Val Loss: 0.6881\n",
            "Epoch: 006, Step: 002, Val Loss: 0.6971\n",
            "Epoch: 006, Step: 003, Val Loss: 0.6990\n",
            "Epoch: 007, Step: 000, Loss: 0.6970\n",
            "Epoch: 007, Step: 001, Loss: 0.6965\n",
            "Epoch: 007, Step: 002, Loss: 0.6991\n",
            "Epoch: 007, Step: 003, Loss: 0.6931\n",
            "Epoch: 007, Step: 004, Loss: 0.7097\n",
            "Epoch: 007, Step: 005, Loss: 0.7075\n",
            "Epoch: 007, Step: 006, Loss: 0.6863\n",
            "Epoch: 007, Step: 007, Loss: 0.7012\n",
            "Epoch: 007, Step: 008, Loss: 0.6921\n",
            "Epoch: 007, Step: 009, Loss: 0.7056\n",
            "Epoch: 007, Step: 010, Loss: 0.7096\n",
            "Epoch: 007, Step: 011, Loss: 0.7017\n",
            "Epoch: 007, Step: 012, Loss: 0.6955\n",
            "Epoch: 007, Step: 013, Loss: 0.7075\n",
            "Epoch: 007, Step: 014, Loss: 0.6976\n",
            "Epoch: 007, Step: 015, Loss: 0.7007\n",
            "Epoch: 007, Step: 016, Loss: 0.7027\n",
            "Epoch: 007, Step: 017, Loss: 0.7041\n",
            "Epoch: 007, Step: 018, Loss: 0.6937\n",
            "Epoch: 007, Step: 019, Loss: 0.6992\n",
            "Epoch: 007, Step: 020, Loss: 0.7007\n",
            "Epoch: 007, Step: 021, Loss: 0.6959\n",
            "Epoch: 007, Step: 022, Loss: 0.6942\n",
            "Epoch: 007, Step: 023, Loss: 0.7019\n",
            "Epoch: 007, Step: 024, Loss: 0.6988\n",
            "Epoch: 007, Step: 025, Loss: 0.6976\n",
            "Epoch: 007, Step: 026, Loss: 0.6956\n",
            "Epoch: 007, Step: 027, Loss: 0.6809\n",
            "Epoch: 007, Step: 028, Loss: 0.6994\n",
            "Epoch: 007, Step: 029, Loss: 0.6867\n",
            "Epoch: 007, Step: 000, Val Loss: 0.6953\n",
            "Epoch: 007, Step: 001, Val Loss: 0.6971\n",
            "Epoch: 007, Step: 002, Val Loss: 0.6917\n",
            "Epoch: 007, Step: 003, Val Loss: 0.6884\n",
            "Epoch: 008, Step: 000, Loss: 0.6966\n",
            "Epoch: 008, Step: 001, Loss: 0.7026\n",
            "Epoch: 008, Step: 002, Loss: 0.6994\n",
            "Epoch: 008, Step: 003, Loss: 0.6842\n",
            "Epoch: 008, Step: 004, Loss: 0.6998\n",
            "Epoch: 008, Step: 005, Loss: 0.6988\n",
            "Epoch: 008, Step: 006, Loss: 0.7078\n",
            "Epoch: 008, Step: 007, Loss: 0.6999\n",
            "Epoch: 008, Step: 008, Loss: 0.7100\n",
            "Epoch: 008, Step: 009, Loss: 0.6920\n",
            "Epoch: 008, Step: 010, Loss: 0.6972\n",
            "Epoch: 008, Step: 011, Loss: 0.6958\n",
            "Epoch: 008, Step: 012, Loss: 0.6981\n",
            "Epoch: 008, Step: 013, Loss: 0.7102\n",
            "Epoch: 008, Step: 014, Loss: 0.6940\n",
            "Epoch: 008, Step: 015, Loss: 0.7132\n",
            "Epoch: 008, Step: 016, Loss: 0.6942\n",
            "Epoch: 008, Step: 017, Loss: 0.6921\n",
            "Epoch: 008, Step: 018, Loss: 0.6914\n",
            "Epoch: 008, Step: 019, Loss: 0.7032\n",
            "Epoch: 008, Step: 020, Loss: 0.7054\n",
            "Epoch: 008, Step: 021, Loss: 0.6873\n",
            "Epoch: 008, Step: 022, Loss: 0.7132\n",
            "Epoch: 008, Step: 023, Loss: 0.6963\n",
            "Epoch: 008, Step: 024, Loss: 0.6984\n",
            "Epoch: 008, Step: 025, Loss: 0.7018\n",
            "Epoch: 008, Step: 026, Loss: 0.6948\n",
            "Epoch: 008, Step: 027, Loss: 0.7011\n",
            "Epoch: 008, Step: 028, Loss: 0.6920\n",
            "Epoch: 008, Step: 029, Loss: 0.6895\n",
            "Epoch: 008, Step: 000, Val Loss: 0.6934\n",
            "Epoch: 008, Step: 001, Val Loss: 0.7039\n",
            "Epoch: 008, Step: 002, Val Loss: 0.6901\n",
            "Epoch: 008, Step: 003, Val Loss: 0.6835\n",
            "Epoch: 009, Step: 000, Loss: 0.6927\n",
            "Epoch: 009, Step: 001, Loss: 0.6996\n",
            "Epoch: 009, Step: 002, Loss: 0.7005\n",
            "Epoch: 009, Step: 003, Loss: 0.7060\n",
            "Epoch: 009, Step: 004, Loss: 0.6787\n",
            "Epoch: 009, Step: 005, Loss: 0.6864\n",
            "Epoch: 009, Step: 006, Loss: 0.7043\n",
            "Epoch: 009, Step: 007, Loss: 0.7038\n",
            "Epoch: 009, Step: 008, Loss: 0.7033\n",
            "Epoch: 009, Step: 009, Loss: 0.6972\n",
            "Epoch: 009, Step: 010, Loss: 0.6899\n",
            "Epoch: 009, Step: 011, Loss: 0.6949\n",
            "Epoch: 009, Step: 012, Loss: 0.7037\n",
            "Epoch: 009, Step: 013, Loss: 0.6902\n",
            "Epoch: 009, Step: 014, Loss: 0.6876\n",
            "Epoch: 009, Step: 015, Loss: 0.6975\n",
            "Epoch: 009, Step: 016, Loss: 0.7031\n",
            "Epoch: 009, Step: 017, Loss: 0.6935\n",
            "Epoch: 009, Step: 018, Loss: 0.6981\n",
            "Epoch: 009, Step: 019, Loss: 0.6913\n",
            "Epoch: 009, Step: 020, Loss: 0.6851\n",
            "Epoch: 009, Step: 021, Loss: 0.6843\n",
            "Epoch: 009, Step: 022, Loss: 0.7044\n",
            "Epoch: 009, Step: 023, Loss: 0.6996\n",
            "Epoch: 009, Step: 024, Loss: 0.7051\n",
            "Epoch: 009, Step: 025, Loss: 0.6964\n",
            "Epoch: 009, Step: 026, Loss: 0.7032\n",
            "Epoch: 009, Step: 027, Loss: 0.7102\n",
            "Epoch: 009, Step: 028, Loss: 0.6990\n",
            "Epoch: 009, Step: 029, Loss: 0.6972\n",
            "Epoch: 009, Step: 000, Val Loss: 0.6901\n",
            "Epoch: 009, Step: 001, Val Loss: 0.6955\n",
            "Epoch: 009, Step: 002, Val Loss: 0.6934\n",
            "Epoch: 009, Step: 003, Val Loss: 0.6956\n",
            "Epoch: 010, Step: 000, Loss: 0.6913\n",
            "Epoch: 010, Step: 001, Loss: 0.6995\n",
            "Epoch: 010, Step: 002, Loss: 0.7019\n",
            "Epoch: 010, Step: 003, Loss: 0.6871\n",
            "Epoch: 010, Step: 004, Loss: 0.7022\n",
            "Epoch: 010, Step: 005, Loss: 0.6864\n",
            "Epoch: 010, Step: 006, Loss: 0.6972\n",
            "Epoch: 010, Step: 007, Loss: 0.6973\n",
            "Epoch: 010, Step: 008, Loss: 0.6984\n",
            "Epoch: 010, Step: 009, Loss: 0.7094\n",
            "Epoch: 010, Step: 010, Loss: 0.6992\n",
            "Epoch: 010, Step: 011, Loss: 0.6929\n",
            "Epoch: 010, Step: 012, Loss: 0.6959\n",
            "Epoch: 010, Step: 013, Loss: 0.7048\n",
            "Epoch: 010, Step: 014, Loss: 0.6918\n",
            "Epoch: 010, Step: 015, Loss: 0.7036\n",
            "Epoch: 010, Step: 016, Loss: 0.7105\n",
            "Epoch: 010, Step: 017, Loss: 0.6918\n",
            "Epoch: 010, Step: 018, Loss: 0.7050\n",
            "Epoch: 010, Step: 019, Loss: 0.7008\n",
            "Epoch: 010, Step: 020, Loss: 0.6920\n",
            "Epoch: 010, Step: 021, Loss: 0.6953\n",
            "Epoch: 010, Step: 022, Loss: 0.7004\n",
            "Epoch: 010, Step: 023, Loss: 0.6994\n",
            "Epoch: 010, Step: 024, Loss: 0.7120\n",
            "Epoch: 010, Step: 025, Loss: 0.7030\n",
            "Epoch: 010, Step: 026, Loss: 0.7010\n",
            "Epoch: 010, Step: 027, Loss: 0.7006\n",
            "Epoch: 010, Step: 028, Loss: 0.6923\n",
            "Epoch: 010, Step: 029, Loss: 0.6856\n",
            "Epoch: 010, Step: 000, Val Loss: 0.6984\n",
            "Epoch: 010, Step: 001, Val Loss: 0.6935\n",
            "Epoch: 010, Step: 002, Val Loss: 0.6868\n",
            "Epoch: 010, Step: 003, Val Loss: 0.6960\n",
            "Epoch: 011, Step: 000, Loss: 0.6894\n",
            "Epoch: 011, Step: 001, Loss: 0.6999\n",
            "Epoch: 011, Step: 002, Loss: 0.6963\n",
            "Epoch: 011, Step: 003, Loss: 0.7073\n",
            "Epoch: 011, Step: 004, Loss: 0.6891\n",
            "Epoch: 011, Step: 005, Loss: 0.7083\n",
            "Epoch: 011, Step: 006, Loss: 0.6945\n",
            "Epoch: 011, Step: 007, Loss: 0.7007\n",
            "Epoch: 011, Step: 008, Loss: 0.7038\n",
            "Epoch: 011, Step: 009, Loss: 0.6886\n",
            "Epoch: 011, Step: 010, Loss: 0.6948\n",
            "Epoch: 011, Step: 011, Loss: 0.7042\n",
            "Epoch: 011, Step: 012, Loss: 0.6968\n",
            "Epoch: 011, Step: 013, Loss: 0.6987\n",
            "Epoch: 011, Step: 014, Loss: 0.7009\n",
            "Epoch: 011, Step: 015, Loss: 0.6901\n",
            "Epoch: 011, Step: 016, Loss: 0.6889\n",
            "Epoch: 011, Step: 017, Loss: 0.6857\n",
            "Epoch: 011, Step: 018, Loss: 0.6931\n",
            "Epoch: 011, Step: 019, Loss: 0.7051\n",
            "Epoch: 011, Step: 020, Loss: 0.6927\n",
            "Epoch: 011, Step: 021, Loss: 0.6878\n",
            "Epoch: 011, Step: 022, Loss: 0.7046\n",
            "Epoch: 011, Step: 023, Loss: 0.6919\n",
            "Epoch: 011, Step: 024, Loss: 0.7043\n",
            "Epoch: 011, Step: 025, Loss: 0.6973\n",
            "Epoch: 011, Step: 026, Loss: 0.7065\n",
            "Epoch: 011, Step: 027, Loss: 0.7126\n",
            "Epoch: 011, Step: 028, Loss: 0.6792\n",
            "Epoch: 011, Step: 029, Loss: 0.7045\n",
            "Epoch: 011, Step: 000, Val Loss: 0.6951\n",
            "Epoch: 011, Step: 001, Val Loss: 0.6967\n",
            "Epoch: 011, Step: 002, Val Loss: 0.6969\n",
            "Epoch: 011, Step: 003, Val Loss: 0.6814\n",
            "Epoch: 012, Step: 000, Loss: 0.6983\n",
            "Epoch: 012, Step: 001, Loss: 0.6997\n",
            "Epoch: 012, Step: 002, Loss: 0.6993\n",
            "Epoch: 012, Step: 003, Loss: 0.7065\n",
            "Epoch: 012, Step: 004, Loss: 0.6874\n",
            "Epoch: 012, Step: 005, Loss: 0.6960\n",
            "Epoch: 012, Step: 006, Loss: 0.7061\n",
            "Epoch: 012, Step: 007, Loss: 0.6891\n",
            "Epoch: 012, Step: 008, Loss: 0.6978\n",
            "Epoch: 012, Step: 009, Loss: 0.6946\n",
            "Epoch: 012, Step: 010, Loss: 0.6998\n",
            "Epoch: 012, Step: 011, Loss: 0.7020\n",
            "Epoch: 012, Step: 012, Loss: 0.7011\n",
            "Epoch: 012, Step: 013, Loss: 0.7039\n",
            "Epoch: 012, Step: 014, Loss: 0.6921\n",
            "Epoch: 012, Step: 015, Loss: 0.7019\n",
            "Epoch: 012, Step: 016, Loss: 0.7059\n",
            "Epoch: 012, Step: 017, Loss: 0.7048\n",
            "Epoch: 012, Step: 018, Loss: 0.6968\n",
            "Epoch: 012, Step: 019, Loss: 0.6947\n",
            "Epoch: 012, Step: 020, Loss: 0.7062\n",
            "Epoch: 012, Step: 021, Loss: 0.6923\n",
            "Epoch: 012, Step: 022, Loss: 0.7027\n",
            "Epoch: 012, Step: 023, Loss: 0.6944\n",
            "Epoch: 012, Step: 024, Loss: 0.6935\n",
            "Epoch: 012, Step: 025, Loss: 0.6882\n",
            "Epoch: 012, Step: 026, Loss: 0.6969\n",
            "Epoch: 012, Step: 027, Loss: 0.6901\n",
            "Epoch: 012, Step: 028, Loss: 0.7101\n",
            "Epoch: 012, Step: 029, Loss: 0.6947\n",
            "Epoch: 012, Step: 000, Val Loss: 0.6933\n",
            "Epoch: 012, Step: 001, Val Loss: 0.6967\n",
            "Epoch: 012, Step: 002, Val Loss: 0.6984\n",
            "Epoch: 012, Step: 003, Val Loss: 0.6817\n",
            "Epoch: 013, Step: 000, Loss: 0.6875\n",
            "Epoch: 013, Step: 001, Loss: 0.6952\n",
            "Epoch: 013, Step: 002, Loss: 0.7064\n",
            "Epoch: 013, Step: 003, Loss: 0.6862\n",
            "Epoch: 013, Step: 004, Loss: 0.6983\n",
            "Epoch: 013, Step: 005, Loss: 0.7041\n",
            "Epoch: 013, Step: 006, Loss: 0.6868\n",
            "Epoch: 013, Step: 007, Loss: 0.6962\n",
            "Epoch: 013, Step: 008, Loss: 0.6990\n",
            "Epoch: 013, Step: 009, Loss: 0.6991\n",
            "Epoch: 013, Step: 010, Loss: 0.7005\n",
            "Epoch: 013, Step: 011, Loss: 0.6997\n",
            "Epoch: 013, Step: 012, Loss: 0.6990\n",
            "Epoch: 013, Step: 013, Loss: 0.7006\n",
            "Epoch: 013, Step: 014, Loss: 0.6990\n",
            "Epoch: 013, Step: 015, Loss: 0.7107\n",
            "Epoch: 013, Step: 016, Loss: 0.6971\n",
            "Epoch: 013, Step: 017, Loss: 0.6975\n",
            "Epoch: 013, Step: 018, Loss: 0.6964\n",
            "Epoch: 013, Step: 019, Loss: 0.6918\n",
            "Epoch: 013, Step: 020, Loss: 0.6991\n",
            "Epoch: 013, Step: 021, Loss: 0.7075\n",
            "Epoch: 013, Step: 022, Loss: 0.6918\n",
            "Epoch: 013, Step: 023, Loss: 0.6910\n",
            "Epoch: 013, Step: 024, Loss: 0.7043\n",
            "Epoch: 013, Step: 025, Loss: 0.6968\n",
            "Epoch: 013, Step: 026, Loss: 0.6894\n",
            "Epoch: 013, Step: 027, Loss: 0.7037\n",
            "Epoch: 013, Step: 028, Loss: 0.6944\n",
            "Epoch: 013, Step: 029, Loss: 0.6960\n",
            "Epoch: 013, Step: 000, Val Loss: 0.6950\n",
            "Epoch: 013, Step: 001, Val Loss: 0.6996\n",
            "Epoch: 013, Step: 002, Val Loss: 0.6888\n",
            "Epoch: 013, Step: 003, Val Loss: 0.6890\n",
            "Epoch: 014, Step: 000, Loss: 0.6932\n",
            "Epoch: 014, Step: 001, Loss: 0.6951\n",
            "Epoch: 014, Step: 002, Loss: 0.7007\n",
            "Epoch: 014, Step: 003, Loss: 0.6983\n",
            "Epoch: 014, Step: 004, Loss: 0.6982\n",
            "Epoch: 014, Step: 005, Loss: 0.7003\n",
            "Epoch: 014, Step: 006, Loss: 0.6950\n",
            "Epoch: 014, Step: 007, Loss: 0.7013\n",
            "Epoch: 014, Step: 008, Loss: 0.6947\n",
            "Epoch: 014, Step: 009, Loss: 0.7030\n",
            "Epoch: 014, Step: 010, Loss: 0.7001\n",
            "Epoch: 014, Step: 011, Loss: 0.7054\n",
            "Epoch: 014, Step: 012, Loss: 0.6974\n",
            "Epoch: 014, Step: 013, Loss: 0.6917\n",
            "Epoch: 014, Step: 014, Loss: 0.6934\n",
            "Epoch: 014, Step: 015, Loss: 0.7052\n",
            "Epoch: 014, Step: 016, Loss: 0.7023\n",
            "Epoch: 014, Step: 017, Loss: 0.7013\n",
            "Epoch: 014, Step: 018, Loss: 0.6986\n",
            "Epoch: 014, Step: 019, Loss: 0.6882\n",
            "Epoch: 014, Step: 020, Loss: 0.7005\n",
            "Epoch: 014, Step: 021, Loss: 0.6919\n",
            "Epoch: 014, Step: 022, Loss: 0.6948\n",
            "Epoch: 014, Step: 023, Loss: 0.6885\n",
            "Epoch: 014, Step: 024, Loss: 0.6958\n",
            "Epoch: 014, Step: 025, Loss: 0.6953\n",
            "Epoch: 014, Step: 026, Loss: 0.6889\n",
            "Epoch: 014, Step: 027, Loss: 0.7009\n",
            "Epoch: 014, Step: 028, Loss: 0.6949\n",
            "Epoch: 014, Step: 029, Loss: 0.7014\n",
            "Epoch: 014, Step: 000, Val Loss: 0.6932\n",
            "Epoch: 014, Step: 001, Val Loss: 0.6981\n",
            "Epoch: 014, Step: 002, Val Loss: 0.6904\n",
            "Epoch: 014, Step: 003, Val Loss: 0.6913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функција за евалуација на моделот со пресметување на точност, прецизност, одзивност и F1 скор\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(batch.y.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # accuracy = accuracy_score(all_labels, all_preds)\n",
        "    # precision = precision_score(all_labels, all_preds, average='binary')\n",
        "    # recall = recall_score(all_labels, all_preds, average='binary')\n",
        "    # f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='micro',zero_division=1)\n",
        "    recall = recall_score(all_labels, all_preds, average='micro',zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_preds, average='micro',zero_division=1)\n",
        "\n",
        "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "fsoTxOFLYJT8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9ykLjM7l-N3",
        "outputId": "6e1a0553-38c2-4d1e-88e7-701cba74ec1d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3076923076923077, Precision: 0.3076923076923077, Recall: 0.3076923076923077, F1 Score: 0.3076923076923077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 2–Класификација на графовисо GCN(25поени) Направетеи втормодел за класификација на графовикој го имплементира GCN. Првиот дел на моделот се 2 или 3 слоеви во GCN, по што се надоврзуваат 1, 2 или 3 скриени FC слоеви за класификација на графовите. Направете класификација на графовитепреку споредба на различните големини и комбинации на слоеви на моделите.Користете ја истата поделба  како во претходната задача. Направете евалуација на истиот начин како во претходната задача.Со кој метод се добиваат подобри резултати?"
      ],
      "metadata": {
        "id": "u2tov08EI0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN моделот од\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(-1, 64)  # SAGEConv\n",
        "        self.conv2 = GCNConv(-1, 128)  # SAGEConv\n",
        "\n",
        "        self.linear1 = Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index).tanh()\n",
        "        x = dropout(x, p=0.3)\n",
        "\n",
        "        x = self.conv2(x, edge_index).tanh()\n",
        "        x = dropout(x, p=0.3)\n",
        "\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        x = dropout(x, p=0.5, training=self.training)\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x, edge_index, batch):\n",
        "    #     # 1. Obtain node embeddings\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = x.relu()\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     x = x.relu()\n",
        "    #     x = self.conv3(x, edge_index)\n",
        "\n",
        "    #     # 2. Readout layer\n",
        "    #     x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "    #     # 3. Apply a final classifier\n",
        "    #     x = dropout(x, p=0.5, training=self.training)\n",
        "    #     x = self.linear1(x)\n",
        "\n",
        "    #     return x\n"
      ],
      "metadata": {
        "id": "CdullsMBK169"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCN_model = GCN(num_classes=dataset.num_classes)\n",
        "GCN_optimizer = SGD(GCN_model.parameters(), lr=0.0001)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "sxgfAB1KN4b3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(GCN_model, train_loader, val_loader, GCN_optimizer, criterion, 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H5X7cwmJOAfg",
        "outputId": "b8e7173a-eb6f-48c8-96f9-139805e3a7ce"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Step: 000, Loss: 0.6751\n",
            "Epoch: 000, Step: 001, Loss: 0.7020\n",
            "Epoch: 000, Step: 002, Loss: 0.6516\n",
            "Epoch: 000, Step: 003, Loss: 0.6777\n",
            "Epoch: 000, Step: 004, Loss: 0.6871\n",
            "Epoch: 000, Step: 005, Loss: 0.7088\n",
            "Epoch: 000, Step: 006, Loss: 0.6959\n",
            "Epoch: 000, Step: 007, Loss: 0.6754\n",
            "Epoch: 000, Step: 008, Loss: 0.6670\n",
            "Epoch: 000, Step: 009, Loss: 0.7006\n",
            "Epoch: 000, Step: 010, Loss: 0.6630\n",
            "Epoch: 000, Step: 011, Loss: 0.6929\n",
            "Epoch: 000, Step: 012, Loss: 0.7031\n",
            "Epoch: 000, Step: 013, Loss: 0.6614\n",
            "Epoch: 000, Step: 014, Loss: 0.6845\n",
            "Epoch: 000, Step: 015, Loss: 0.6777\n",
            "Epoch: 000, Step: 016, Loss: 0.7005\n",
            "Epoch: 000, Step: 017, Loss: 0.6746\n",
            "Epoch: 000, Step: 018, Loss: 0.6951\n",
            "Epoch: 000, Step: 019, Loss: 0.6949\n",
            "Epoch: 000, Step: 020, Loss: 0.6989\n",
            "Epoch: 000, Step: 021, Loss: 0.6845\n",
            "Epoch: 000, Step: 022, Loss: 0.6699\n",
            "Epoch: 000, Step: 023, Loss: 0.6981\n",
            "Epoch: 000, Step: 024, Loss: 0.6723\n",
            "Epoch: 000, Step: 025, Loss: 0.6730\n",
            "Epoch: 000, Step: 026, Loss: 0.6904\n",
            "Epoch: 000, Step: 027, Loss: 0.6844\n",
            "Epoch: 000, Step: 028, Loss: 0.6985\n",
            "Epoch: 000, Step: 029, Loss: 0.6800\n",
            "Epoch: 000, Step: 000, Val Loss: 0.6996\n",
            "Epoch: 000, Step: 001, Val Loss: 0.6833\n",
            "Epoch: 000, Step: 002, Val Loss: 0.7109\n",
            "Epoch: 000, Step: 003, Val Loss: 0.6825\n",
            "Epoch: 001, Step: 000, Loss: 0.6837\n",
            "Epoch: 001, Step: 001, Loss: 0.6910\n",
            "Epoch: 001, Step: 002, Loss: 0.6624\n",
            "Epoch: 001, Step: 003, Loss: 0.6809\n",
            "Epoch: 001, Step: 004, Loss: 0.6746\n",
            "Epoch: 001, Step: 005, Loss: 0.6702\n",
            "Epoch: 001, Step: 006, Loss: 0.6924\n",
            "Epoch: 001, Step: 007, Loss: 0.6870\n",
            "Epoch: 001, Step: 008, Loss: 0.6819\n",
            "Epoch: 001, Step: 009, Loss: 0.6815\n",
            "Epoch: 001, Step: 010, Loss: 0.6921\n",
            "Epoch: 001, Step: 011, Loss: 0.6848\n",
            "Epoch: 001, Step: 012, Loss: 0.6856\n",
            "Epoch: 001, Step: 013, Loss: 0.6845\n",
            "Epoch: 001, Step: 014, Loss: 0.6829\n",
            "Epoch: 001, Step: 015, Loss: 0.6753\n",
            "Epoch: 001, Step: 016, Loss: 0.6959\n",
            "Epoch: 001, Step: 017, Loss: 0.6849\n",
            "Epoch: 001, Step: 018, Loss: 0.6825\n",
            "Epoch: 001, Step: 019, Loss: 0.6986\n",
            "Epoch: 001, Step: 020, Loss: 0.6580\n",
            "Epoch: 001, Step: 021, Loss: 0.6785\n",
            "Epoch: 001, Step: 022, Loss: 0.6903\n",
            "Epoch: 001, Step: 023, Loss: 0.6985\n",
            "Epoch: 001, Step: 024, Loss: 0.6924\n",
            "Epoch: 001, Step: 025, Loss: 0.6839\n",
            "Epoch: 001, Step: 026, Loss: 0.6711\n",
            "Epoch: 001, Step: 027, Loss: 0.7031\n",
            "Epoch: 001, Step: 028, Loss: 0.6653\n",
            "Epoch: 001, Step: 029, Loss: 0.6781\n",
            "Epoch: 001, Step: 000, Val Loss: 0.6899\n",
            "Epoch: 001, Step: 001, Val Loss: 0.6954\n",
            "Epoch: 001, Step: 002, Val Loss: 0.6871\n",
            "Epoch: 001, Step: 003, Val Loss: 0.7143\n",
            "Epoch: 002, Step: 000, Loss: 0.6786\n",
            "Epoch: 002, Step: 001, Loss: 0.6969\n",
            "Epoch: 002, Step: 002, Loss: 0.6797\n",
            "Epoch: 002, Step: 003, Loss: 0.6804\n",
            "Epoch: 002, Step: 004, Loss: 0.6831\n",
            "Epoch: 002, Step: 005, Loss: 0.6864\n",
            "Epoch: 002, Step: 006, Loss: 0.7082\n",
            "Epoch: 002, Step: 007, Loss: 0.6786\n",
            "Epoch: 002, Step: 008, Loss: 0.6848\n",
            "Epoch: 002, Step: 009, Loss: 0.6870\n",
            "Epoch: 002, Step: 010, Loss: 0.6806\n",
            "Epoch: 002, Step: 011, Loss: 0.6943\n",
            "Epoch: 002, Step: 012, Loss: 0.6754\n",
            "Epoch: 002, Step: 013, Loss: 0.6674\n",
            "Epoch: 002, Step: 014, Loss: 0.7037\n",
            "Epoch: 002, Step: 015, Loss: 0.6615\n",
            "Epoch: 002, Step: 016, Loss: 0.6788\n",
            "Epoch: 002, Step: 017, Loss: 0.6813\n",
            "Epoch: 002, Step: 018, Loss: 0.6938\n",
            "Epoch: 002, Step: 019, Loss: 0.6781\n",
            "Epoch: 002, Step: 020, Loss: 0.6738\n",
            "Epoch: 002, Step: 021, Loss: 0.6881\n",
            "Epoch: 002, Step: 022, Loss: 0.6947\n",
            "Epoch: 002, Step: 023, Loss: 0.6793\n",
            "Epoch: 002, Step: 024, Loss: 0.6911\n",
            "Epoch: 002, Step: 025, Loss: 0.6669\n",
            "Epoch: 002, Step: 026, Loss: 0.6802\n",
            "Epoch: 002, Step: 027, Loss: 0.6831\n",
            "Epoch: 002, Step: 028, Loss: 0.6919\n",
            "Epoch: 002, Step: 029, Loss: 0.6741\n",
            "Epoch: 002, Step: 000, Val Loss: 0.6996\n",
            "Epoch: 002, Step: 001, Val Loss: 0.6908\n",
            "Epoch: 002, Step: 002, Val Loss: 0.6946\n",
            "Epoch: 002, Step: 003, Val Loss: 0.6958\n",
            "Epoch: 003, Step: 000, Loss: 0.6865\n",
            "Epoch: 003, Step: 001, Loss: 0.7129\n",
            "Epoch: 003, Step: 002, Loss: 0.6934\n",
            "Epoch: 003, Step: 003, Loss: 0.6778\n",
            "Epoch: 003, Step: 004, Loss: 0.6923\n",
            "Epoch: 003, Step: 005, Loss: 0.6901\n",
            "Epoch: 003, Step: 006, Loss: 0.7036\n",
            "Epoch: 003, Step: 007, Loss: 0.6568\n",
            "Epoch: 003, Step: 008, Loss: 0.6840\n",
            "Epoch: 003, Step: 009, Loss: 0.6631\n",
            "Epoch: 003, Step: 010, Loss: 0.6714\n",
            "Epoch: 003, Step: 011, Loss: 0.7005\n",
            "Epoch: 003, Step: 012, Loss: 0.6716\n",
            "Epoch: 003, Step: 013, Loss: 0.6707\n",
            "Epoch: 003, Step: 014, Loss: 0.6788\n",
            "Epoch: 003, Step: 015, Loss: 0.6806\n",
            "Epoch: 003, Step: 016, Loss: 0.6915\n",
            "Epoch: 003, Step: 017, Loss: 0.6978\n",
            "Epoch: 003, Step: 018, Loss: 0.6962\n",
            "Epoch: 003, Step: 019, Loss: 0.6712\n",
            "Epoch: 003, Step: 020, Loss: 0.6851\n",
            "Epoch: 003, Step: 021, Loss: 0.6998\n",
            "Epoch: 003, Step: 022, Loss: 0.6694\n",
            "Epoch: 003, Step: 023, Loss: 0.6800\n",
            "Epoch: 003, Step: 024, Loss: 0.6921\n",
            "Epoch: 003, Step: 025, Loss: 0.6869\n",
            "Epoch: 003, Step: 026, Loss: 0.6917\n",
            "Epoch: 003, Step: 027, Loss: 0.6730\n",
            "Epoch: 003, Step: 028, Loss: 0.6876\n",
            "Epoch: 003, Step: 029, Loss: 0.6883\n",
            "Epoch: 003, Step: 000, Val Loss: 0.6901\n",
            "Epoch: 003, Step: 001, Val Loss: 0.7240\n",
            "Epoch: 003, Step: 002, Val Loss: 0.6881\n",
            "Epoch: 003, Step: 003, Val Loss: 0.6712\n",
            "Epoch: 004, Step: 000, Loss: 0.6925\n",
            "Epoch: 004, Step: 001, Loss: 0.6959\n",
            "Epoch: 004, Step: 002, Loss: 0.6701\n",
            "Epoch: 004, Step: 003, Loss: 0.6776\n",
            "Epoch: 004, Step: 004, Loss: 0.6711\n",
            "Epoch: 004, Step: 005, Loss: 0.6991\n",
            "Epoch: 004, Step: 006, Loss: 0.6549\n",
            "Epoch: 004, Step: 007, Loss: 0.6635\n",
            "Epoch: 004, Step: 008, Loss: 0.6873\n",
            "Epoch: 004, Step: 009, Loss: 0.6892\n",
            "Epoch: 004, Step: 010, Loss: 0.6800\n",
            "Epoch: 004, Step: 011, Loss: 0.7001\n",
            "Epoch: 004, Step: 012, Loss: 0.6958\n",
            "Epoch: 004, Step: 013, Loss: 0.6806\n",
            "Epoch: 004, Step: 014, Loss: 0.6877\n",
            "Epoch: 004, Step: 015, Loss: 0.7003\n",
            "Epoch: 004, Step: 016, Loss: 0.6839\n",
            "Epoch: 004, Step: 017, Loss: 0.6927\n",
            "Epoch: 004, Step: 018, Loss: 0.6980\n",
            "Epoch: 004, Step: 019, Loss: 0.6731\n",
            "Epoch: 004, Step: 020, Loss: 0.6837\n",
            "Epoch: 004, Step: 021, Loss: 0.6778\n",
            "Epoch: 004, Step: 022, Loss: 0.6813\n",
            "Epoch: 004, Step: 023, Loss: 0.7019\n",
            "Epoch: 004, Step: 024, Loss: 0.6901\n",
            "Epoch: 004, Step: 025, Loss: 0.6965\n",
            "Epoch: 004, Step: 026, Loss: 0.6720\n",
            "Epoch: 004, Step: 027, Loss: 0.6674\n",
            "Epoch: 004, Step: 028, Loss: 0.6694\n",
            "Epoch: 004, Step: 029, Loss: 0.6898\n",
            "Epoch: 004, Step: 000, Val Loss: 0.6957\n",
            "Epoch: 004, Step: 001, Val Loss: 0.6860\n",
            "Epoch: 004, Step: 002, Val Loss: 0.6994\n",
            "Epoch: 004, Step: 003, Val Loss: 0.7025\n",
            "Epoch: 005, Step: 000, Loss: 0.6825\n",
            "Epoch: 005, Step: 001, Loss: 0.6874\n",
            "Epoch: 005, Step: 002, Loss: 0.6649\n",
            "Epoch: 005, Step: 003, Loss: 0.7037\n",
            "Epoch: 005, Step: 004, Loss: 0.6826\n",
            "Epoch: 005, Step: 005, Loss: 0.7035\n",
            "Epoch: 005, Step: 006, Loss: 0.6692\n",
            "Epoch: 005, Step: 007, Loss: 0.6904\n",
            "Epoch: 005, Step: 008, Loss: 0.6742\n",
            "Epoch: 005, Step: 009, Loss: 0.6766\n",
            "Epoch: 005, Step: 010, Loss: 0.6699\n",
            "Epoch: 005, Step: 011, Loss: 0.7024\n",
            "Epoch: 005, Step: 012, Loss: 0.6971\n",
            "Epoch: 005, Step: 013, Loss: 0.6902\n",
            "Epoch: 005, Step: 014, Loss: 0.6824\n",
            "Epoch: 005, Step: 015, Loss: 0.6824\n",
            "Epoch: 005, Step: 016, Loss: 0.7003\n",
            "Epoch: 005, Step: 017, Loss: 0.6814\n",
            "Epoch: 005, Step: 018, Loss: 0.6791\n",
            "Epoch: 005, Step: 019, Loss: 0.6891\n",
            "Epoch: 005, Step: 020, Loss: 0.7043\n",
            "Epoch: 005, Step: 021, Loss: 0.6614\n",
            "Epoch: 005, Step: 022, Loss: 0.6733\n",
            "Epoch: 005, Step: 023, Loss: 0.6816\n",
            "Epoch: 005, Step: 024, Loss: 0.6867\n",
            "Epoch: 005, Step: 025, Loss: 0.6891\n",
            "Epoch: 005, Step: 026, Loss: 0.6743\n",
            "Epoch: 005, Step: 027, Loss: 0.6763\n",
            "Epoch: 005, Step: 028, Loss: 0.6912\n",
            "Epoch: 005, Step: 029, Loss: 0.6752\n",
            "Epoch: 005, Step: 000, Val Loss: 0.7072\n",
            "Epoch: 005, Step: 001, Val Loss: 0.6620\n",
            "Epoch: 005, Step: 002, Val Loss: 0.7298\n",
            "Epoch: 005, Step: 003, Val Loss: 0.6766\n",
            "Epoch: 006, Step: 000, Loss: 0.6860\n",
            "Epoch: 006, Step: 001, Loss: 0.6924\n",
            "Epoch: 006, Step: 002, Loss: 0.6789\n",
            "Epoch: 006, Step: 003, Loss: 0.6861\n",
            "Epoch: 006, Step: 004, Loss: 0.6911\n",
            "Epoch: 006, Step: 005, Loss: 0.6943\n",
            "Epoch: 006, Step: 006, Loss: 0.6929\n",
            "Epoch: 006, Step: 007, Loss: 0.6753\n",
            "Epoch: 006, Step: 008, Loss: 0.6753\n",
            "Epoch: 006, Step: 009, Loss: 0.6787\n",
            "Epoch: 006, Step: 010, Loss: 0.6715\n",
            "Epoch: 006, Step: 011, Loss: 0.6972\n",
            "Epoch: 006, Step: 012, Loss: 0.6746\n",
            "Epoch: 006, Step: 013, Loss: 0.6960\n",
            "Epoch: 006, Step: 014, Loss: 0.6886\n",
            "Epoch: 006, Step: 015, Loss: 0.6719\n",
            "Epoch: 006, Step: 016, Loss: 0.6920\n",
            "Epoch: 006, Step: 017, Loss: 0.6804\n",
            "Epoch: 006, Step: 018, Loss: 0.6890\n",
            "Epoch: 006, Step: 019, Loss: 0.6863\n",
            "Epoch: 006, Step: 020, Loss: 0.6736\n",
            "Epoch: 006, Step: 021, Loss: 0.6863\n",
            "Epoch: 006, Step: 022, Loss: 0.6672\n",
            "Epoch: 006, Step: 023, Loss: 0.6808\n",
            "Epoch: 006, Step: 024, Loss: 0.6881\n",
            "Epoch: 006, Step: 025, Loss: 0.6915\n",
            "Epoch: 006, Step: 026, Loss: 0.6951\n",
            "Epoch: 006, Step: 027, Loss: 0.6734\n",
            "Epoch: 006, Step: 028, Loss: 0.6809\n",
            "Epoch: 006, Step: 029, Loss: 0.6694\n",
            "Epoch: 006, Step: 000, Val Loss: 0.7049\n",
            "Epoch: 006, Step: 001, Val Loss: 0.6996\n",
            "Epoch: 006, Step: 002, Val Loss: 0.6858\n",
            "Epoch: 006, Step: 003, Val Loss: 0.6894\n",
            "Epoch: 007, Step: 000, Loss: 0.7029\n",
            "Epoch: 007, Step: 001, Loss: 0.6667\n",
            "Epoch: 007, Step: 002, Loss: 0.6856\n",
            "Epoch: 007, Step: 003, Loss: 0.7062\n",
            "Epoch: 007, Step: 004, Loss: 0.6750\n",
            "Epoch: 007, Step: 005, Loss: 0.6892\n",
            "Epoch: 007, Step: 006, Loss: 0.6951\n",
            "Epoch: 007, Step: 007, Loss: 0.6744\n",
            "Epoch: 007, Step: 008, Loss: 0.6686\n",
            "Epoch: 007, Step: 009, Loss: 0.6763\n",
            "Epoch: 007, Step: 010, Loss: 0.6765\n",
            "Epoch: 007, Step: 011, Loss: 0.6893\n",
            "Epoch: 007, Step: 012, Loss: 0.6928\n",
            "Epoch: 007, Step: 013, Loss: 0.6959\n",
            "Epoch: 007, Step: 014, Loss: 0.6681\n",
            "Epoch: 007, Step: 015, Loss: 0.6692\n",
            "Epoch: 007, Step: 016, Loss: 0.6854\n",
            "Epoch: 007, Step: 017, Loss: 0.6862\n",
            "Epoch: 007, Step: 018, Loss: 0.6657\n",
            "Epoch: 007, Step: 019, Loss: 0.6844\n",
            "Epoch: 007, Step: 020, Loss: 0.6954\n",
            "Epoch: 007, Step: 021, Loss: 0.6826\n",
            "Epoch: 007, Step: 022, Loss: 0.7012\n",
            "Epoch: 007, Step: 023, Loss: 0.6733\n",
            "Epoch: 007, Step: 024, Loss: 0.6878\n",
            "Epoch: 007, Step: 025, Loss: 0.6856\n",
            "Epoch: 007, Step: 026, Loss: 0.6738\n",
            "Epoch: 007, Step: 027, Loss: 0.6700\n",
            "Epoch: 007, Step: 028, Loss: 0.7057\n",
            "Epoch: 007, Step: 029, Loss: 0.6778\n",
            "Epoch: 007, Step: 000, Val Loss: 0.6993\n",
            "Epoch: 007, Step: 001, Val Loss: 0.6963\n",
            "Epoch: 007, Step: 002, Val Loss: 0.6994\n",
            "Epoch: 007, Step: 003, Val Loss: 0.6824\n",
            "Epoch: 008, Step: 000, Loss: 0.6754\n",
            "Epoch: 008, Step: 001, Loss: 0.6910\n",
            "Epoch: 008, Step: 002, Loss: 0.6908\n",
            "Epoch: 008, Step: 003, Loss: 0.6631\n",
            "Epoch: 008, Step: 004, Loss: 0.6986\n",
            "Epoch: 008, Step: 005, Loss: 0.6956\n",
            "Epoch: 008, Step: 006, Loss: 0.6853\n",
            "Epoch: 008, Step: 007, Loss: 0.6665\n",
            "Epoch: 008, Step: 008, Loss: 0.6827\n",
            "Epoch: 008, Step: 009, Loss: 0.6992\n",
            "Epoch: 008, Step: 010, Loss: 0.7045\n",
            "Epoch: 008, Step: 011, Loss: 0.6752\n",
            "Epoch: 008, Step: 012, Loss: 0.6841\n",
            "Epoch: 008, Step: 013, Loss: 0.6774\n",
            "Epoch: 008, Step: 014, Loss: 0.6955\n",
            "Epoch: 008, Step: 015, Loss: 0.6916\n",
            "Epoch: 008, Step: 016, Loss: 0.6749\n",
            "Epoch: 008, Step: 017, Loss: 0.6732\n",
            "Epoch: 008, Step: 018, Loss: 0.6822\n",
            "Epoch: 008, Step: 019, Loss: 0.6893\n",
            "Epoch: 008, Step: 020, Loss: 0.6694\n",
            "Epoch: 008, Step: 021, Loss: 0.7117\n",
            "Epoch: 008, Step: 022, Loss: 0.6829\n",
            "Epoch: 008, Step: 023, Loss: 0.6735\n",
            "Epoch: 008, Step: 024, Loss: 0.6805\n",
            "Epoch: 008, Step: 025, Loss: 0.6802\n",
            "Epoch: 008, Step: 026, Loss: 0.6811\n",
            "Epoch: 008, Step: 027, Loss: 0.6853\n",
            "Epoch: 008, Step: 028, Loss: 0.6700\n",
            "Epoch: 008, Step: 029, Loss: 0.6666\n",
            "Epoch: 008, Step: 000, Val Loss: 0.6919\n",
            "Epoch: 008, Step: 001, Val Loss: 0.6993\n",
            "Epoch: 008, Step: 002, Val Loss: 0.6953\n",
            "Epoch: 008, Step: 003, Val Loss: 0.6946\n",
            "Epoch: 009, Step: 000, Loss: 0.7003\n",
            "Epoch: 009, Step: 001, Loss: 0.6699\n",
            "Epoch: 009, Step: 002, Loss: 0.6832\n",
            "Epoch: 009, Step: 003, Loss: 0.7003\n",
            "Epoch: 009, Step: 004, Loss: 0.6991\n",
            "Epoch: 009, Step: 005, Loss: 0.6754\n",
            "Epoch: 009, Step: 006, Loss: 0.6788\n",
            "Epoch: 009, Step: 007, Loss: 0.6757\n",
            "Epoch: 009, Step: 008, Loss: 0.6793\n",
            "Epoch: 009, Step: 009, Loss: 0.6804\n",
            "Epoch: 009, Step: 010, Loss: 0.6859\n",
            "Epoch: 009, Step: 011, Loss: 0.7008\n",
            "Epoch: 009, Step: 012, Loss: 0.6659\n",
            "Epoch: 009, Step: 013, Loss: 0.6839\n",
            "Epoch: 009, Step: 014, Loss: 0.6642\n",
            "Epoch: 009, Step: 015, Loss: 0.6734\n",
            "Epoch: 009, Step: 016, Loss: 0.6849\n",
            "Epoch: 009, Step: 017, Loss: 0.6928\n",
            "Epoch: 009, Step: 018, Loss: 0.7115\n",
            "Epoch: 009, Step: 019, Loss: 0.6959\n",
            "Epoch: 009, Step: 020, Loss: 0.6812\n",
            "Epoch: 009, Step: 021, Loss: 0.6833\n",
            "Epoch: 009, Step: 022, Loss: 0.6810\n",
            "Epoch: 009, Step: 023, Loss: 0.6835\n",
            "Epoch: 009, Step: 024, Loss: 0.6751\n",
            "Epoch: 009, Step: 025, Loss: 0.7040\n",
            "Epoch: 009, Step: 026, Loss: 0.6846\n",
            "Epoch: 009, Step: 027, Loss: 0.6721\n",
            "Epoch: 009, Step: 028, Loss: 0.6639\n",
            "Epoch: 009, Step: 029, Loss: 0.6860\n",
            "Epoch: 009, Step: 000, Val Loss: 0.6913\n",
            "Epoch: 009, Step: 001, Val Loss: 0.6917\n",
            "Epoch: 009, Step: 002, Val Loss: 0.7038\n",
            "Epoch: 009, Step: 003, Val Loss: 0.6953\n",
            "Epoch: 010, Step: 000, Loss: 0.6680\n",
            "Epoch: 010, Step: 001, Loss: 0.6881\n",
            "Epoch: 010, Step: 002, Loss: 0.6841\n",
            "Epoch: 010, Step: 003, Loss: 0.7022\n",
            "Epoch: 010, Step: 004, Loss: 0.6996\n",
            "Epoch: 010, Step: 005, Loss: 0.6694\n",
            "Epoch: 010, Step: 006, Loss: 0.6882\n",
            "Epoch: 010, Step: 007, Loss: 0.7015\n",
            "Epoch: 010, Step: 008, Loss: 0.6834\n",
            "Epoch: 010, Step: 009, Loss: 0.6844\n",
            "Epoch: 010, Step: 010, Loss: 0.6707\n",
            "Epoch: 010, Step: 011, Loss: 0.6886\n",
            "Epoch: 010, Step: 012, Loss: 0.6834\n",
            "Epoch: 010, Step: 013, Loss: 0.6889\n",
            "Epoch: 010, Step: 014, Loss: 0.6899\n",
            "Epoch: 010, Step: 015, Loss: 0.6863\n",
            "Epoch: 010, Step: 016, Loss: 0.6596\n",
            "Epoch: 010, Step: 017, Loss: 0.7131\n",
            "Epoch: 010, Step: 018, Loss: 0.6855\n",
            "Epoch: 010, Step: 019, Loss: 0.6853\n",
            "Epoch: 010, Step: 020, Loss: 0.6756\n",
            "Epoch: 010, Step: 021, Loss: 0.6830\n",
            "Epoch: 010, Step: 022, Loss: 0.6846\n",
            "Epoch: 010, Step: 023, Loss: 0.6697\n",
            "Epoch: 010, Step: 024, Loss: 0.6917\n",
            "Epoch: 010, Step: 025, Loss: 0.6767\n",
            "Epoch: 010, Step: 026, Loss: 0.6820\n",
            "Epoch: 010, Step: 027, Loss: 0.6722\n",
            "Epoch: 010, Step: 028, Loss: 0.6712\n",
            "Epoch: 010, Step: 029, Loss: 0.6508\n",
            "Epoch: 010, Step: 000, Val Loss: 0.6960\n",
            "Epoch: 010, Step: 001, Val Loss: 0.7084\n",
            "Epoch: 010, Step: 002, Val Loss: 0.6827\n",
            "Epoch: 010, Step: 003, Val Loss: 0.6948\n",
            "Epoch: 011, Step: 000, Loss: 0.6978\n",
            "Epoch: 011, Step: 001, Loss: 0.6964\n",
            "Epoch: 011, Step: 002, Loss: 0.6950\n",
            "Epoch: 011, Step: 003, Loss: 0.6849\n",
            "Epoch: 011, Step: 004, Loss: 0.6875\n",
            "Epoch: 011, Step: 005, Loss: 0.6772\n",
            "Epoch: 011, Step: 006, Loss: 0.6515\n",
            "Epoch: 011, Step: 007, Loss: 0.6833\n",
            "Epoch: 011, Step: 008, Loss: 0.6875\n",
            "Epoch: 011, Step: 009, Loss: 0.6706\n",
            "Epoch: 011, Step: 010, Loss: 0.6693\n",
            "Epoch: 011, Step: 011, Loss: 0.6771\n",
            "Epoch: 011, Step: 012, Loss: 0.6802\n",
            "Epoch: 011, Step: 013, Loss: 0.6877\n",
            "Epoch: 011, Step: 014, Loss: 0.7031\n",
            "Epoch: 011, Step: 015, Loss: 0.6752\n",
            "Epoch: 011, Step: 016, Loss: 0.6762\n",
            "Epoch: 011, Step: 017, Loss: 0.6696\n",
            "Epoch: 011, Step: 018, Loss: 0.6905\n",
            "Epoch: 011, Step: 019, Loss: 0.6780\n",
            "Epoch: 011, Step: 020, Loss: 0.6972\n",
            "Epoch: 011, Step: 021, Loss: 0.7034\n",
            "Epoch: 011, Step: 022, Loss: 0.6882\n",
            "Epoch: 011, Step: 023, Loss: 0.6808\n",
            "Epoch: 011, Step: 024, Loss: 0.6873\n",
            "Epoch: 011, Step: 025, Loss: 0.6926\n",
            "Epoch: 011, Step: 026, Loss: 0.6845\n",
            "Epoch: 011, Step: 027, Loss: 0.6795\n",
            "Epoch: 011, Step: 028, Loss: 0.6704\n",
            "Epoch: 011, Step: 029, Loss: 0.6818\n",
            "Epoch: 011, Step: 000, Val Loss: 0.6696\n",
            "Epoch: 011, Step: 001, Val Loss: 0.6950\n",
            "Epoch: 011, Step: 002, Val Loss: 0.7137\n",
            "Epoch: 011, Step: 003, Val Loss: 0.7075\n",
            "Epoch: 012, Step: 000, Loss: 0.6737\n",
            "Epoch: 012, Step: 001, Loss: 0.6630\n",
            "Epoch: 012, Step: 002, Loss: 0.6480\n",
            "Epoch: 012, Step: 003, Loss: 0.7066\n",
            "Epoch: 012, Step: 004, Loss: 0.6918\n",
            "Epoch: 012, Step: 005, Loss: 0.6819\n",
            "Epoch: 012, Step: 006, Loss: 0.6842\n",
            "Epoch: 012, Step: 007, Loss: 0.6730\n",
            "Epoch: 012, Step: 008, Loss: 0.6897\n",
            "Epoch: 012, Step: 009, Loss: 0.6958\n",
            "Epoch: 012, Step: 010, Loss: 0.6950\n",
            "Epoch: 012, Step: 011, Loss: 0.7011\n",
            "Epoch: 012, Step: 012, Loss: 0.7042\n",
            "Epoch: 012, Step: 013, Loss: 0.6745\n",
            "Epoch: 012, Step: 014, Loss: 0.6822\n",
            "Epoch: 012, Step: 015, Loss: 0.6835\n",
            "Epoch: 012, Step: 016, Loss: 0.6645\n",
            "Epoch: 012, Step: 017, Loss: 0.6967\n",
            "Epoch: 012, Step: 018, Loss: 0.6920\n",
            "Epoch: 012, Step: 019, Loss: 0.6933\n",
            "Epoch: 012, Step: 020, Loss: 0.6773\n",
            "Epoch: 012, Step: 021, Loss: 0.6734\n",
            "Epoch: 012, Step: 022, Loss: 0.7018\n",
            "Epoch: 012, Step: 023, Loss: 0.6871\n",
            "Epoch: 012, Step: 024, Loss: 0.6687\n",
            "Epoch: 012, Step: 025, Loss: 0.6736\n",
            "Epoch: 012, Step: 026, Loss: 0.6804\n",
            "Epoch: 012, Step: 027, Loss: 0.6883\n",
            "Epoch: 012, Step: 028, Loss: 0.6810\n",
            "Epoch: 012, Step: 029, Loss: 0.6719\n",
            "Epoch: 012, Step: 000, Val Loss: 0.6872\n",
            "Epoch: 012, Step: 001, Val Loss: 0.6911\n",
            "Epoch: 012, Step: 002, Val Loss: 0.7172\n",
            "Epoch: 012, Step: 003, Val Loss: 0.6818\n",
            "Epoch: 013, Step: 000, Loss: 0.6818\n",
            "Epoch: 013, Step: 001, Loss: 0.6768\n",
            "Epoch: 013, Step: 002, Loss: 0.6715\n",
            "Epoch: 013, Step: 003, Loss: 0.6809\n",
            "Epoch: 013, Step: 004, Loss: 0.6900\n",
            "Epoch: 013, Step: 005, Loss: 0.6734\n",
            "Epoch: 013, Step: 006, Loss: 0.6807\n",
            "Epoch: 013, Step: 007, Loss: 0.6791\n",
            "Epoch: 013, Step: 008, Loss: 0.6862\n",
            "Epoch: 013, Step: 009, Loss: 0.6723\n",
            "Epoch: 013, Step: 010, Loss: 0.6932\n",
            "Epoch: 013, Step: 011, Loss: 0.6914\n",
            "Epoch: 013, Step: 012, Loss: 0.6743\n",
            "Epoch: 013, Step: 013, Loss: 0.6872\n",
            "Epoch: 013, Step: 014, Loss: 0.7062\n",
            "Epoch: 013, Step: 015, Loss: 0.6728\n",
            "Epoch: 013, Step: 016, Loss: 0.6905\n",
            "Epoch: 013, Step: 017, Loss: 0.6981\n",
            "Epoch: 013, Step: 018, Loss: 0.6786\n",
            "Epoch: 013, Step: 019, Loss: 0.7012\n",
            "Epoch: 013, Step: 020, Loss: 0.6830\n",
            "Epoch: 013, Step: 021, Loss: 0.6714\n",
            "Epoch: 013, Step: 022, Loss: 0.7015\n",
            "Epoch: 013, Step: 023, Loss: 0.6751\n",
            "Epoch: 013, Step: 024, Loss: 0.6597\n",
            "Epoch: 013, Step: 025, Loss: 0.6745\n",
            "Epoch: 013, Step: 026, Loss: 0.6899\n",
            "Epoch: 013, Step: 027, Loss: 0.6800\n",
            "Epoch: 013, Step: 028, Loss: 0.6798\n",
            "Epoch: 013, Step: 029, Loss: 0.7071\n",
            "Epoch: 013, Step: 000, Val Loss: 0.6863\n",
            "Epoch: 013, Step: 001, Val Loss: 0.7111\n",
            "Epoch: 013, Step: 002, Val Loss: 0.6927\n",
            "Epoch: 013, Step: 003, Val Loss: 0.6898\n",
            "Epoch: 014, Step: 000, Loss: 0.7007\n",
            "Epoch: 014, Step: 001, Loss: 0.6644\n",
            "Epoch: 014, Step: 002, Loss: 0.7059\n",
            "Epoch: 014, Step: 003, Loss: 0.6853\n",
            "Epoch: 014, Step: 004, Loss: 0.6729\n",
            "Epoch: 014, Step: 005, Loss: 0.6792\n",
            "Epoch: 014, Step: 006, Loss: 0.6865\n",
            "Epoch: 014, Step: 007, Loss: 0.6701\n",
            "Epoch: 014, Step: 008, Loss: 0.6950\n",
            "Epoch: 014, Step: 009, Loss: 0.6503\n",
            "Epoch: 014, Step: 010, Loss: 0.6972\n",
            "Epoch: 014, Step: 011, Loss: 0.6764\n",
            "Epoch: 014, Step: 012, Loss: 0.6832\n",
            "Epoch: 014, Step: 013, Loss: 0.6805\n",
            "Epoch: 014, Step: 014, Loss: 0.6635\n",
            "Epoch: 014, Step: 015, Loss: 0.6983\n",
            "Epoch: 014, Step: 016, Loss: 0.6907\n",
            "Epoch: 014, Step: 017, Loss: 0.6988\n",
            "Epoch: 014, Step: 018, Loss: 0.6819\n",
            "Epoch: 014, Step: 019, Loss: 0.6757\n",
            "Epoch: 014, Step: 020, Loss: 0.6767\n",
            "Epoch: 014, Step: 021, Loss: 0.6822\n",
            "Epoch: 014, Step: 022, Loss: 0.6933\n",
            "Epoch: 014, Step: 023, Loss: 0.6752\n",
            "Epoch: 014, Step: 024, Loss: 0.6902\n",
            "Epoch: 014, Step: 025, Loss: 0.7064\n",
            "Epoch: 014, Step: 026, Loss: 0.6756\n",
            "Epoch: 014, Step: 027, Loss: 0.6783\n",
            "Epoch: 014, Step: 028, Loss: 0.7001\n",
            "Epoch: 014, Step: 029, Loss: 0.6946\n",
            "Epoch: 014, Step: 000, Val Loss: 0.6956\n",
            "Epoch: 014, Step: 001, Val Loss: 0.6785\n",
            "Epoch: 014, Step: 002, Val Loss: 0.7044\n",
            "Epoch: 014, Step: 003, Val Loss: 0.7076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(GCN_model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8QnWe97QtKd",
        "outputId": "78bee2f1-aba2-4c4d-abf6-09e390dc1457"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6923076923076923, Precision: 0.6923076923076923, Recall: 0.6923076923076923, F1 Score: 0.6923076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кроз сите метрики GCN дава подобри резултати, така да GCN е подобар.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "GraphSAGE резултати:\n",
        "\n",
        "Accuracy: 0.3076923076923077, Precision: 0.3076923076923077, Recall: 0.3076923076923077, F1 Score: 0.3076923076923077\n",
        "\n",
        "\n",
        "---\n",
        "GCN резулати:\n",
        "\n",
        "Accuracy: 0.6923076923076923, Precision: 0.6923076923076923, Recall: 0.6923076923076923, F1 Score: 0.6923076923076923\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qiT2Z87QZ3Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGeSaWT_c6Et"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}